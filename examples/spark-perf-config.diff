--- spark-perf-master/config/config.py.template	2015-12-09 07:21:03.000000000 +0900
+++ spark-perf-config-basic.py	2016-07-26 21:50:26.000000000 +0900
@@ -18,7 +18,7 @@
 # ================================ #
 
 # Point to an installation of Spark on the cluster.
-SPARK_HOME_DIR = "/root/spark"
+SPARK_HOME_DIR = "/opt/aics/spark/spark-1.6.2-bin-sparkk"
 
 # Use a custom configuration directory
 SPARK_CONF_DIR = SPARK_HOME_DIR + "/conf"
@@ -27,7 +27,8 @@
 # For local clusters: "spark://%s:7077" % socket.gethostname()
 # For Yarn clusters: "yarn"
 # Otherwise, the default uses the specified EC2 cluster
-SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()
+#SPARK_CLUSTER_URL = open("/root/spark-ec2/cluster-url", 'r').readline().strip()
+SPARK_CLUSTER_URL = "spark://%s:7077" % socket.gethostname()
 IS_YARN_MODE = "yarn" in SPARK_CLUSTER_URL
 IS_MESOS_MODE = "mesos" in SPARK_CLUSTER_URL
 
@@ -47,7 +48,9 @@
 USE_CLUSTER_SPARK = True
 
 # URL of the HDFS installation in the Spark EC2 cluster
-HDFS_URL = "hdfs://%s:9000/test/" % socket.gethostname()
+#HDFS_URL = "hdfs://%s:9000/test/" % socket.gethostname()
+HDFS_URL = "{}/test/".format(os.environ["PJM_JOBDIR"])
+
 
 # Set the following if not using existing Spark installation
 # Commit id and repo used if you are not using an existing Spark cluster
@@ -69,14 +72,14 @@
 
 # Whether to restart the Master and all Workers
 # This should always be false for Yarn
-RESTART_SPARK_CLUSTER = True
+RESTART_SPARK_CLUSTER = False
 RESTART_SPARK_CLUSTER = RESTART_SPARK_CLUSTER and not IS_YARN_MODE
 
 # Rsync SPARK_HOME to all the slaves or not
 RSYNC_SPARK_HOME = True
 
 # Which tests to run
-RUN_SPARK_TESTS = True
+RUN_SPARK_TESTS = False
 RUN_PYSPARK_TESTS = False
 RUN_STREAMING_TESTS = False
 RUN_MLLIB_TESTS = False
@@ -84,7 +87,7 @@
 
 # Which tests to prepare. Set this to true for the first
 # installation or whenever you make a change to the tests.
-PREP_SPARK_TESTS = True
+PREP_SPARK_TESTS = False
 PREP_PYSPARK_TESTS = False
 PREP_STREAMING_TESTS = False
 PREP_MLLIB_TESTS = False
@@ -100,7 +103,7 @@
 DISK_WARMUP_FILES = 200
 
 # Prompt for confirmation when deleting temporary files.
-PROMPT_FOR_DELETES = True
+PROMPT_FOR_DELETES = False
 
 # Files to write results to
 SPARK_OUTPUT_FILENAME = "results/spark_perf_output_%s_%s" % (
@@ -124,7 +127,8 @@
 # number of records in a generated dataset) if you are running the tests with more
 # or fewer nodes. When developing new test suites, you might want to set this to a small
 # value suitable for a single machine, such as 0.001.
-SCALE_FACTOR = 1.0
+#SCALE_FACTOR = 1.0
+SCALE_FACTOR = 0.5
 
 assert SCALE_FACTOR > 0, "SCALE_FACTOR must be > 0."
 
@@ -141,7 +145,7 @@
 # Java options.
 COMMON_JAVA_OPTS = [
     # Fraction of JVM memory used for caching RDDs.
-    JavaOptionSet("spark.storage.memoryFraction", [0.66]),
+    #JavaOptionSet("spark.storage.memoryFraction", [0.66]),
     JavaOptionSet("spark.serializer", ["org.apache.spark.serializer.JavaSerializer"]),
     # JavaOptionSet("spark.executor.memory", ["9g"]),
     # Turn event logging on in order better diagnose failed tests. Off by default as it crashes
@@ -151,7 +155,7 @@
     JavaOptionSet("spark.locality.wait", [str(60 * 1000 * 1000)])
 ]
 # Set driver memory here
-SPARK_DRIVER_MEMORY = "20g"
+SPARK_DRIVER_MEMORY = "10g"
 # The following options value sets are shared among all tests.
 COMMON_OPTS = [
     # How many times to run each experiment - used to warm up system caches.
@@ -167,9 +171,11 @@
 # operations on key-value data.
 SPARK_KEY_VAL_TEST_OPTS = [
     # The number of input partitions.
-    OptionSet("num-partitions", [400], can_scale=True),
+    #OptionSet("num-partitions", [400], can_scale=True),
+    OptionSet("num-partitions", [100], can_scale=True),
     # The number of reduce tasks.
-    OptionSet("reduce-tasks", [400], can_scale=True),
+    #OptionSet("reduce-tasks", [400], can_scale=True),
+    OptionSet("reduce-tasks", [100], can_scale=True),
     # A random seed to make tests reproducable.
     OptionSet("random-seed", [5]),
     # Input persistence strategy (can be "memory", "disk", or "hdfs").
@@ -192,8 +198,7 @@
     # Use hashes instead of padded numbers for keys and values
     FlagSet("hash-records", [False]),
     # Storage location if HDFS persistence is used
-    OptionSet("storage-location", [
-        HDFS_URL + "/spark-perf-kv-data"])
+    OptionSet("storage-location", [HDFS_URL + "/spark-perf-kv-data"])
 ]
 
 
