#!/bin/sh

#
# Spark on K intialize script
#

curdir=`pwd`
# move to the home directory
cd

#
# K Job system forcibly exports POSIXY_CORRECT, so unset it.
#
unset POSIXLY_CORRECT

. /work/system/Env_base 1> /dev/null 2>&1

mpiexec /work/system/bin/msh hostname > k_nodes 2> /dev/null
export NODES_NUBMER=`wc -l k_nodes 2> /dev/null`

if [ "${NODES_NUBMER}" -gt 1 ] && [ ! -z "${SEPARATE_DRIVER_NODE}" -a "${SEPARATE_DRIVER_NODE}" == "y" ]; then
    K_DRIVER_NODE="`hotaname`"
    K_MASTER_NODE="`grep -v \`hostname\` k_nodes | head -n 1`"
else
    K_MASTER_NODE="`hostname`"
    K_DRIVER_NODE=""
fi

COMMON_DIR=`(cd ..; pwd)`
SPARK_ARCHIVE=spark-1.6.0-bin-custom-spark

mkdir ${COMMON_DIR}/conf
cat << EOS > ${COMMON_DIR}/conf/spark-env.sh
. /work/system/Env_base 1> /dev/null 2>&1
EOS

if [ -d /opt/spark ]; then
  SPARK_HOME=/opt/spark
else if [ -d /opt/local/spark ]; then
  SPARK_HOME=/opt/local/spark
else if [ -d /opt/aics/spark ]; then
  SPARK_HOME=/opt/aics/spark
else if [ -f ./spark-1.6.0-bin-custom-spark.tgz ]; then
  # Expects Spark tarball exists
  tar zxf ${SPARK_ARCHIVE}.tgz
  mv ${SPARK_ARCHIVE} ../
  SPARK_HOME=`pwd`/../${SPARK_ARCHIVE}
  SPARK_HOME=`(cd ${SPARK_HOME}; pwd)`
else
  # Expects spark-submit is in PATH
  spark_sumit=`which spark-submit 2> /dev/null`
  if [ $? -eq 0 ]; then
    dir=`dirname $spark_sumit`
    SPARK_HOME=`cd $dir; ..`
  fi
fi; fi; fi; fi

if [ -z "$SPARK_HOME" ]; then
  echo "Spark doesn't exists"
  exit 1
fi

# Spark configuration
export SPARK_HOME
export JAVA_HOME=/opt/klocal/openjdk7u45
export PATH=${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:${JAVA_HOME}/jre/lib:${JAVA_HOME}/lib:${JAVA_HOME}/lib/tools.jar
export SPARK_MASTER_NODE=spark://${K_MASTER_NODE}:7077

# Spark environments those passed to Spark master and worker
SPARK_ENV="SPARK_CONF_DIR=${COMMON_DIR}/conf \
SPARK_LOCAL_DIRS=\${rankdir}/localdir \
SPARK_WORKER_DIR=\${rankdir}/work \
SPARK_LOG_DIR=\${rankdir}/logs \
SPARK_PID_DIR=\${rankdir}/pids "

# Python configuration
export PYTHON_HOME=/opt/local/Python-2.7.3/bin
export PATH=${PYTHON_HOME}/bin:${PATH}
export LD_LIBRARY_PATH=${PYTHON_HOME}/lib:${LD_LIBRARY_PATH}

# R configuration
export RHOME=/opt/aics/R
export PATH=${RHOME}/bin:$PATH

# Other configuration
export COMMON_DIR

cat << EOF > $${COMMON_DIR}/start-spark.sh
#!/usr/bin/env bash

unset POSIXLY_CORRECT

rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${SPARK_MASTER_NODE}" ]; then
    ${SPARK_ENV} ${SPARK_HOME}/sbin/start-master.sh
fi

if [ \${hostname} != ${K_DRIVER_NODE} -a \${hostname} != ${K_MASTER_NODE} ] || \
   [ \${hostname} == ${K_DRIVER_NODE} -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname} == ${K_MASTER_NODE} -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname} == ${K_MASTER_NODE} -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then

   ${SPARK_ENV} ${SPARK_HOME}/sbin/start-slave.sh
fi

if [ "\${hostname}" == "${SPARK_MASTER_NODE}" ]; then
   ${SPARK_K}/spark-k-wait-initialize
fi

EOF

cat << EOF > ${COMMON_DIR}/stop-spark.sh
#!/usr/bin/env bash

unset POSIXLY_CORRECT

rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${SPARK_MASTER_NODE}" ]; then
   ${SPARK_K}/spark-k-wait-spark-job-finish
fi

if [ \${hostname} != ${K_DRIVER_NODE} -a \${hostname} != ${K_MASTER_NODE} ] \\
   [ \${hostname} == ${K_DRIVER_NODE} -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname} == ${K_MASTER_NODE} -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname} == ${K_MASTER_NODE} -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then

    ${SPARK_ENV} ${SPARK_HOME}/sbin/stop-slave.sh
fi

if [ "\${hostname}" == "${SPARK_MASTER_NODE}" ]; then
    ${SPARK_ENV} ${SPARK_HOME}/sbin/stop-master.sh
fi
EOF

mpiexec /work/system/bin/msh ${COMMON_DIR}/start-spark.sh

cd ${curdir}
