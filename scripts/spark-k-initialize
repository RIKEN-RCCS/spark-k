#!/bin/sh

#
# Script to initialize the Spark environment on K
#


#
# K Job system forcibly exports POSIXY_CORRECT, so unset it.
#
unset POSIXLY_CORRECT

#
# Spark on K intialize script
#

K_NODES_FILE=.k_nodes
K_WORKER_NODES_FILE=.k_worker_nodes
SPARK_ARCHIVE=spark-1.6.0-bin-custom-spark

_curdir=`pwd`
cd ${PJM_JOBDIR}
COMMON_DIR=`pwd`

if [ -d "${PJM_JOBDIR}/0" ]; then
    cd ${PJM_JOBDIR}/0
fi
_rankdir=`pwd`

. /work/system/Env_base 1> /dev/null 2>&1

mkdir -p ${COMMON_DIR}/hosts

cat << EOF > ${COMMON_DIR}/gen_hosts.sh
#!/bin/sh

touch ${COMMON_DIR}/hosts/\`hostname\`
sync
EOF

if [ ! -f ${K_NODES_FILE} ]; then
    mpiexec /work/system/bin/msh sh ${COMMON_DIR}/gen_hosts.sh
    (cd ${COMMON_DIR}/hosts; ls -1) > ${K_NODES_FILE}
fi

NODES_NUBMER=`wc -l ${K_NODES_FILE} 2>&1 | awk '{print $1}' 2> /dev/null`
if [ "${NODES_NUBMER}" -gt 1 ] && [ ! -z "${SEPARATE_DRIVER_NODE}" ]; then
    K_DRIVER_NODE="`hostname`"
    K_MASTER_NODE="`grep -v \`hostname\` ${K_NODES_FILE} | head -n 1`"
    if [ "${NODES_NUBMER}" -gt 2 ]; then
        _workers_number=`expr $NODES_NUBMER - 2`
    else
        _workers_number=1
    fi
else
    K_MASTER_NODE="`hostname`"
    K_DRIVER_NODE=""
    if [ "${NODES_NUBMER}" -gt 1 ]; then
        _workers_number=`expr $NODES_NUBMER - 1`
    else
        _workers_number=1
    fi
fi

cat ${K_NODES_FILE} |\
 grep -v ${K_MASTER_NODE} |\
 ([ -z "${K_DRIVER_NODE}" ] && cat || grep -v ${K_DRIVER_NODE}) > ${COMMON_DIR}/${K_WORKER_NODES_FILE}
if [ ! -z ${K_WORKER_NODES_NUMBER} ]; then
    head -n ${K_WORKER_NODES_NUMBER} ${COMMON_DIR}/${K_WORKER_NODES_FILE} > ${COMMON_DIR}/${K_WORKER_NODES_FILE}.tmp
    mv ${COMMON_DIR}/${K_WORKER_NODES_FILE}.tmp ${COMMON_DIR}/${K_WORKER_NODES_FILE}
    if [ ${K_WORKER_NODES_NUMBER} -lt ${_workers_number} ]; then
        _workers_number=${K_WORKER_NODES_NUMBER}
    fi
fi

mkdir -p ${COMMON_DIR}/conf
cat << EOS > ${COMMON_DIR}/conf/spark-env.sh
. /work/system/Env_base 1> /dev/null 2>&1
EOS

if [ -d /opt/spark ]; then
  SPARK_HOME=/opt/spark
else if [ -d /opt/local/spark ]; then
  SPARK_HOME=/opt/local/spark
else if [ -d /opt/aics/spark ]; then
  SPARK_HOME=/opt/aics/spark
else if [ -f ./${SPARK_ARCHIVE}.tgz ]; then
  # Expects Spark tarball exists
  SPARK_HOME=${COMMON_DIR}/${SPARK_ARCHIVE}
  if [ ! -d  ${SPARK_HOME} ]; then
    tar zxf ${SPARK_ARCHIVE}.tgz
    mv ${SPARK_ARCHIVE} ${COMMON_DIR}/ 
  fi
  SPARK_HOME=`(cd ${SPARK_HOME}; pwd)`
else
  # Expects spark-submit is in PATH
  spark_submit=`which spark-submit 2> /dev/null`
  if [ $? -eq 0 ]; then
    dir=`dirname ${spark_sumit}`
    SPARK_HOME=`(cd ${dir}; pwd)`
  fi
fi; fi; fi; fi

if [ -z "$SPARK_HOME" ]; then
  echo "Spark doesn't exists"
  exit 1
fi

# Spark configuration
export SPARK_HOME
export JAVA_HOME=/opt/klocal/openjdk7u45
export PATH=${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:${JAVA_HOME}/jre/lib:${JAVA_HOME}/lib:${JAVA_HOME}/lib/tools.jar
export SPARK_MASTER=spark://${K_MASTER_NODE}:7077

# Spark environments those passed to Spark master and worker
# NOTE: non-null SPARK_MASTER environment variable cause rsync in start-master/slave.sh
SPARK_ENV="SPARK_CONF_DIR=${COMMON_DIR}/conf \
SPARK_LOCAL_DIRS=\${_rankdir}/localdir \
SPARK_WORKER_DIR=\${_rankdir}/work \
SPARK_LOG_DIR=\${_rankdir}/logs \
SPARK_PID_DIR=\${_rankdir}/pids \
SPARK_MASTER= "

# Spark environments those passed to Spark master
SPARK_MASTER_ENV="${SPARK_ENV} \
SPARK_DAEMON_MEMORY=4g"

# Spark environments those passed to Spark worker
SPARK_WORKER_ENV="${SPARK_ENV} \
SPARK_DAEMON_MEMORY=1g"


# Python configuration
export PYTHON_HOME=/opt/local/Python-2.7.3
export PATH=${PYTHON_HOME}/bin:${PATH}
export LD_LIBRARY_PATH=${PYTHON_HOME}/lib:${LD_LIBRARY_PATH}

# R configuration
export RHOME=/opt/aics/R
export PATH=${RHOME}/bin:$PATH

# Other configuration
export COMMON_DIR


cat << EOF > ${COMMON_DIR}/start-spark.sh
#!/usr/bin/env bash

set -x

unset POSIXLY_CORRECT

_rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
    ${SPARK_MASTER_ENV} ${SPARK_HOME}/sbin/start-master.sh
fi

if [ \${hostname}x != ${K_DRIVER_NODE}x -a \${hostname}x != ${K_MASTER_NODE}x ] || \
   [ \${hostname}x == ${K_DRIVER_NODE}x -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then
    if [ \`grep -c \${hostname} ${COMMON_DIR}/${K_WORKER_NODES_FILE} \` -eq 1 ]; then
        ${SPARK_WORKER_ENV} ${SPARK_HOME}/sbin/start-slave.sh ${SPARK_MASTER}
    fi
fi

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
   ${SPARK_K}spark-k-wait-initialize --host ${K_MASTER_NODE} --node-num ${_workers_number}
fi
EOF
chmod +x ${COMMON_DIR}/start-spark.sh

cat << EOF > ${COMMON_DIR}/stop-spark.sh
#!/usr/bin/env bash

set -x

unset POSIXLY_CORRECT

_rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
    ${SPARK_K}spark-k-wait-spark-job-finish
fi

if [ \${hostname}x != ${K_DRIVER_NODE}x -a \${hostname} != ${K_MASTER_NODE} ] || \
   [ \${hostname}x == ${K_DRIVER_NODE}x -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then
    if [ \`grep -c \${hostname} ${COMMON_DIR}/${K_WORKER_NODES_FILE} \` -eq 1 ]; then
        ${SPARK_WORKER_ENV} ${SPARK_HOME}/sbin/stop-slave.sh
    fi
fi

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
    ${SPARK_MASTER_ENV} ${SPARK_HOME}/sbin/stop-master.sh
fi
EOF
chmod +x ${COMMON_DIR}/stop-spark.sh

mpiexec /work/system/bin/msh ${COMMON_DIR}/start-spark.sh

cd ${_curdir}
