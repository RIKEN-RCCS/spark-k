#!/bin/sh

#
# Spark on K intialize script
#

curdir=`pwd`
# move to the home directory
cd

#
# K Job system forcibly exports POSIXY_CORRECT, so unset it.
#
unset POSIXLY_CORRECT

. /work/system/Env_base 1> /dev/null 2>&1

mpiexec /work/system/bin/msh hostname > k_nodes 2> /dev/null
NODES_NUBMER=`wc -l k_nodes | awk '{print $1}' 2> /dev/null`

if [ "${NODES_NUBMER}" -gt 1 ] && [ ! -z "${SEPARATE_DRIVER_NODE}" -a "${SEPARATE_DRIVER_NODE}" == "y" ]; then
    K_DRIVER_NODE="`hotaname`"
    K_MASTER_NODE="`grep -v \`hostname\` k_nodes | head -n 1`"
    if [ "${NODES_NUBMER}" -gt 2 ]; then
        WORKERS_NUMBER=`expr $NODES_NUBMER - 2`
    else
        WORKERS_NUMBER=1
    fi
else
    K_MASTER_NODE="`hostname`"
    K_DRIVER_NODE=""
    if [ "${NODES_NUBMER}" -gt 1 ]; then
        WORKERS_NUMBER=`expr $NODES_NUBMER - 1`
    else
        WORKERS_NUMBER=1
    fi
fi

rankdir=`pwd`
if [ "0" == "`basename ${rankdir}`" ]; then
    COMMON_DIR=`(cd ..; pwd)`
else
    COMMON_DIR=`pwd`
fi
SPARK_ARCHIVE=spark-1.6.0-bin-custom-spark

mkdir -p ${COMMON_DIR}/conf
cat << EOS > ${COMMON_DIR}/conf/spark-env.sh
. /work/system/Env_base 1> /dev/null 2>&1
EOS

if [ -d /opt/spark ]; then
  SPARK_HOME=/opt/spark
else if [ -d /opt/local/spark ]; then
  SPARK_HOME=/opt/local/spark
else if [ -d /opt/aics/spark ]; then
  SPARK_HOME=/opt/aics/spark
else if [ -f ./${SPARK_ARCHIVE}.tgz ]; then
  # Expects Spark tarball exists
  SPARK_HOME=${COMMON_DIR}/${SPARK_ARCHIVE}
  if [ ! -d  ${SPARK_HOME} ]; then
    tar zxf ${SPARK_ARCHIVE}.tgz
    mv ${SPARK_ARCHIVE} ${COMMON_DIR}/ 
  fi
  SPARK_HOME=`(cd ${SPARK_HOME}; pwd)`
else
  # Expects spark-submit is in PATH
  spark_submit=`which spark-submit 2> /dev/null`
  if [ $? -eq 0 ]; then
    dir=`dirname ${spark_sumit}`
    SPARK_HOME=`(cd ${dir}; pwd)`
  fi
fi; fi; fi; fi

if [ -z "$SPARK_HOME" ]; then
  echo "Spark doesn't exists"
  exit 1
fi

# Spark configuration
export SPARK_HOME
export JAVA_HOME=/opt/klocal/openjdk7u45
export PATH=${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:${JAVA_HOME}/jre/lib:${JAVA_HOME}/lib:${JAVA_HOME}/lib/tools.jar
export SPARK_MASTER=spark://${K_MASTER_NODE}:7077

# Spark environments those passed to Spark master and worker
# NOTE: non-null SPARK_MASTER environment variable cause rsync in start-master/slave.sh
SPARK_ENV="SPARK_CONF_DIR=${COMMON_DIR}/conf \
SPARK_LOCAL_DIRS=\${rankdir}/localdir \
SPARK_WORKER_DIR=\${rankdir}/work \
SPARK_LOG_DIR=\${rankdir}/logs \
SPARK_PID_DIR=\${rankdir}/pids \
SPARK_MASTER= "

# Python configuration
export PYTHON_HOME=/opt/local/Python-2.7.3
export PATH=${PYTHON_HOME}/bin:${PATH}
export LD_LIBRARY_PATH=${PYTHON_HOME}/lib:${LD_LIBRARY_PATH}

# R configuration
export RHOME=/opt/aics/R
export PATH=${RHOME}/bin:$PATH

# Other configuration
export COMMON_DIR


cat << EOF > ${COMMON_DIR}/start-spark.sh
#!/usr/bin/env bash

set -x

unset POSIXLY_CORRECT

rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
    ${SPARK_ENV} ${SPARK_HOME}/sbin/start-master.sh
fi

if [ \${hostname}x != ${K_DRIVER_NODE}x -a \${hostname}x != ${K_MASTER_NODE}x ] || \
   [ \${hostname}x == ${K_DRIVER_NODE}x -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then

   ${SPARK_ENV} ${SPARK_HOME}/sbin/start-slave.sh ${SPARK_MASTER}
fi

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
   master_log="\${rankdir}/logs/\`ls -1tr \${rankdir}/logs | grep 'Master' | tail -n 1\`"
   ${SPARK_K}spark-k-wait-initialize --node-num ${WORKERS_NUMBER} \${master_log}
fi
EOF
chmod +x ${COMMON_DIR}/start-spark.sh

cat << EOF > ${COMMON_DIR}/stop-spark.sh
#!/usr/bin/env bash

set -x

unset POSIXLY_CORRECT

rankdir=\`pwd\`

. /work/system/Env_base 1> /dev/null 2>&1

# Spark Configurations
export SPARK_HOME=${SPARK_HOME}
export JAVA_HOME=${JAVA_HOME}
export PATH=\${JAVA_HOME}/bin:$PATH
export CLASSPATH=.:\${JAVA_HOME}/jre/lib:\${JAVA_HOME}/lib:\${JAVA_HOME}/lib/tools.jar

# R Configurations
export RHOME=${RHOME}
export PATH=\${RHOME}/bin:$PATH

# Python Configurations
export PYTHON_HOME=${PYTHON_HOME}
export PATH=\${PYTHON_HOME}/bin:\${PATH}
export LD_LIBRARY_PATH=\${PYTHON_HOME}/lib:\${LD_LIBRARY_PATH}

hostname=\`hostname\`

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
   ${SPARK_K}spark-k-wait-spark-job-finish
fi

if [ \${hostname}x != ${K_DRIVER_NODE}x -a \${hostname} != ${K_MASTER_NODE} ] || \
   [ \${hostname}x == ${K_DRIVER_NODE}x -a ${NODES_NUBMER} -eq 1 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a ! -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 2 ] || \
   [ \${hostname}x == ${K_MASTER_NODE}x -a -z "${K_DRIVER_NODE}" -a ${NODES_NUBMER} -eq 1 ]; then

    ${SPARK_ENV} ${SPARK_HOME}/sbin/stop-slave.sh
fi

if [ "\${hostname}" == "${K_MASTER_NODE}" ]; then
    ${SPARK_ENV} ${SPARK_HOME}/sbin/stop-master.sh
fi
EOF
chmod +x ${COMMON_DIR}/stop-spark.sh

mpiexec /work/system/bin/msh ${COMMON_DIR}/start-spark.sh

cd ${curdir}
